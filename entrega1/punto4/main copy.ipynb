{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67829275",
   "metadata": {},
   "source": [
    "# Primer Job \n",
    "Escribe en el archivo format para cada jugador su puntaje promedio y su puntaje historico nuevo y anterior (en la primera vuelta son iguales) y si alguien lo reto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b0e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MRE import Job\n",
    "inputDir = \"./partidas/\"\n",
    "promedioDir = \"../punto2/contabilizado/\"\n",
    "outputDir = \"./format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1818.2631578947369\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m jobCount.setShuffleCmp(cmpShort)\n\u001b[32m     44\u001b[39m jobCount.setSortCmp(cmpShort)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m success = \u001b[43mjobCount\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitForCompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(success)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BigData/entrega1/punto4/MRE.py:473\u001b[39m, in \u001b[36mJob.waitForCompletion\u001b[39m\u001b[34m(this)\u001b[39m\n\u001b[32m    471\u001b[39m this.__shuffle(context)\n\u001b[32m    472\u001b[39m this.__sort(context)\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m \u001b[43mthis\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m context.finish()\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BigData/entrega1/punto4/MRE.py:466\u001b[39m, in \u001b[36mJob.__reduce\u001b[39m\u001b[34m(this, context)\u001b[39m\n\u001b[32m    464\u001b[39m context.startReduce()\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (k,vs) \u001b[38;5;129;01min\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[43mthis\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__fReduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mfred\u001b[39m\u001b[34m(key, values, context)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Estructura esperada: [PP_i , PH_viejo]\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(estructura)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m pp_i = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mestructura\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     31\u001b[39m ph_viejo = \u001b[38;5;28mfloat\u001b[39m(estructura[\u001b[32m1\u001b[39m])\n\u001b[32m     32\u001b[39m retador = v[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def fmapBase(key, value, context):\n",
    "    lista = value.split()\n",
    "    ph = 1\n",
    "    context.write((int(key), \"PP\"), ((float(lista[0]) + 1)/(float(lista[1])+1),ph))\n",
    "\n",
    "def fmapPartida(retador, value, context):\n",
    "    lista = value.split(\"\\t\")\n",
    "    retado = lista[0]\n",
    "    context.write((int(retado), \"SENT TO\"), int(retador))\n",
    "\n",
    "def cmpShort(key1,key2):\n",
    "    if(key1[0] == key2[0]):\n",
    "        return 0\n",
    "    elif(key1[0] < key2[0]):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def cmpShuffle(key1, key2):\n",
    "    if(key1[1] == key2[1]):\n",
    "        return 0\n",
    "    elif(key1[1] == \"PP\"): #PP y PH es el primero \n",
    "        return -1\n",
    "    else:\n",
    "        return 1 #Caso Contribucion\n",
    "    \n",
    "def fred(key, values, context):\n",
    "    estructura = next(values) \n",
    "    # Estructura esperada: [PP_i , PH_viejo]\n",
    "    pp_i = float(estructura[0])\n",
    "    retador = v[0]\n",
    "\n",
    "    for v in values: \n",
    "        context.write(key, (pp_i, puntaje_heroico_inicial, puntaje_heroico_inicial, retador))\n",
    "\n",
    "    \n",
    "puntaje_heroico_inicial = 1\n",
    "\n",
    "jobCount = Job(promedioDir, outputDir, fmapBase, fred)\n",
    "jobCount.setParams(puntaje_heroico_inicial)\n",
    "jobCount.addInputPath(inputDir, fmapPartida)\n",
    "jobCount.setShuffleCmp(cmpShort)\n",
    "jobCount.setSortCmp(cmpShort)\n",
    "\n",
    "success = jobCount.waitForCompletion()\n",
    "\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875849c3",
   "metadata": {},
   "source": [
    "# Segundo job  = iterador\n",
    "Toma la salida del job anterior para saber los datos de cada jugador contribuir al calculo de quienes lo retaron (si nadie lo reto no envia info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffe0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Iteración 0 - MAE: 1257.551749\n",
      ">>> Iteración 1 - MAE: 1812.838400\n",
      ">>> Iteración 2 - MAE: 1812.838400\n",
      ">>> Iteración 3 - MAE: 1812.838400\n",
      ">>> Iteración 4 - MAE: 1812.838400\n",
      ">>> Iteración 5 - MAE: 1812.838400\n",
      ">>> Iteración 6 - MAE: 1812.838400\n",
      ">>> Iteración 7 - MAE: 1812.838400\n",
      ">>> Iteración 8 - MAE: 1812.838400\n",
      ">>> Iteración 9 - MAE: 1812.838400\n",
      ">>> Iteración 10 - MAE: 1812.838400\n",
      ">>> Iteración 11 - MAE: 1812.838400\n",
      ">>> Iteración 12 - MAE: 1812.838400\n",
      ">>> Iteración 13 - MAE: 1812.838400\n",
      ">>> Iteración 14 - MAE: 1812.838400\n",
      ">>> Iteración 15 - MAE: 1812.838400\n",
      ">>> Iteración 16 - MAE: 1812.838400\n",
      ">>> Iteración 17 - MAE: 1812.838400\n",
      ">>> Iteración 18 - MAE: 1812.838400\n",
      ">>> Iteración 19 - MAE: 1812.838400\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fmapJugador(key, value, context):\n",
    "    lista = value.split(\"\\t\")\n",
    "    promedio = lista[0]\n",
    "    ph_inicial = lista[1]\n",
    "    retador = lista[2]\n",
    "    context.write((int(key), \"DATOS\"), (promedio, ph_inicial))\n",
    "    if retador != \"\":  # Evitar casos sin retadores\n",
    "        context.write((int(retador), \"CONTRIBUCION\"), (promedio, ph_inicial))\n",
    "\n",
    "def cmpShort(key1,key2):\n",
    "    if(key1[0] == key2[0]):\n",
    "        return 0\n",
    "    elif(key1[0] < key2[0]):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def cmpShuffle(key1, key2):\n",
    "    if(key1[1] == key2[1]):\n",
    "        return 0\n",
    "    elif(key1[1] == \"DATOS\"): #PP y PH es el primero \n",
    "        return -1\n",
    "    else:\n",
    "        return 1 #Caso Contribucion\n",
    "\n",
    "def fred(key, values, context):\n",
    "    estructura = next(values) \n",
    "    # Estructura esperada: [PP_i , PH_viejo]\n",
    "    \n",
    "    pp_i = float(estructura[0])\n",
    "    ph_viejo = float(estructura[1])\n",
    "    \n",
    "    \n",
    "    acumulador = 0\n",
    "    for v in values:\n",
    "        # v viene del Mapper como (PP_j, PH_j)\n",
    "        pp_j = float(v[0])\n",
    "        ph_j = float(v[1])\n",
    "        acumulador += ph_j * (pp_i / pp_j)\n",
    "    \n",
    "    nuevo_ph = (alfa * acumulador) + (1 - alfa)\n",
    "    \n",
    "    # Emitimos manteniendo la estructura para la siguiente iteración\n",
    "    # Formato: ID \\t PP \\t {Retados} \\t PH_Nuevo\n",
    "    context.write(key, (pp_i, nuevo_ph, ph_viejo))\n",
    "\n",
    "# JOB INTERMEDIO QUE SOLO CALCULA EL MAE ENTRE DOS ITERACIONES\n",
    "def map_mae(key, value, context):\n",
    "    # Asumiendo que la estructura de value es: [PP, {Retados}, PH_Nuevo, PH_Viejo]\n",
    "    # Si 'value' viene de disco como string, hay que parsearlo\n",
    "    datos = value.split(\"\\t\")\n",
    "        \n",
    "    ph_nuevo = float(datos[2])\n",
    "    ph_viejo = float(datos[3])\n",
    "    \n",
    "    # Calculamos el error absoluto: |PH_nuevo - PH_viejo|\n",
    "    error_absoluto = abs(ph_nuevo - ph_viejo)\n",
    "    \n",
    "    # Enviamos a una clave fija para que un Reducer calcule el promedio final\n",
    "    # Emitimos (error, 1) para poder calcular el promedio (Suma / Total)\n",
    "    context.write(\"GLOBAL_MAE\", (error_absoluto, 1))\n",
    "\n",
    "def red_mae(key, values, context):\n",
    "    suma_error_absoluto = 0\n",
    "    conteo_jugadores = 0\n",
    "    \n",
    "    for err, count in values:\n",
    "        suma_error_absoluto += err\n",
    "        conteo_jugadores += count\n",
    "    \n",
    "    # MAE = Suma de errores absolutos / N\n",
    "    mae_final = suma_error_absoluto / conteo_jugadores if conteo_jugadores > 0 else 0\n",
    "    \n",
    "    context.write(\"RESULTADO_MAE\", mae_final)\n",
    "\n",
    "\n",
    "# Parámetros iniciales\n",
    "alfa = 0.1\n",
    "error_tolerado = 0.001\n",
    "max_iter = 20\n",
    "input_dir = \"./format/\" # Salida del Job anterior (ID \\t [PP, PH, {Retados}])\n",
    "\n",
    "for i in range(max_iter):\n",
    "    output_dir = f\"./iteracion_{i}/\"\n",
    "    \n",
    "    # Configuramos el Job\n",
    "    job = Job(input_dir, output_dir, fmap, fred)\n",
    "    job.setParams(alfa)\n",
    "    job.setShuffleCmp(cmpShort)\n",
    "    job.setSortCmp(cmpShuffle)\n",
    "    \n",
    "    success = job.waitForCompletion()\n",
    "    \n",
    "    # --- CÁLCULO DE CONVERGENCIA (MSE) ---\n",
    "    mae_dir = f\"./mae_iter_{i}/\"\n",
    "    \n",
    "    # 1. Ejecutar Job de cálculo de PH (el que ya tienes)\n",
    "    # 2. Ejecutar Job de MAE\n",
    "    jobMAE = Job(output_dir, mae_dir, map_mae, red_mae)\n",
    "    jobMAE.waitForCompletion()\n",
    "    \n",
    "    # 3. Leer el resultado del MAE desde el Driver\n",
    "    # (El driver solo lee un archivo de texto de pocos bytes)\n",
    "    try:\n",
    "        with open(f\"{mae_dir}/output.txt\", \"r\") as f:\n",
    "            # Formato esperado: RESULTADO_MAE \\t 0.00045\n",
    "            linea = f.read().split(\"\\t\")\n",
    "            mae_actual = float(linea[1])\n",
    "            \n",
    "        print(f\">>> Iteración {i} - MAE: {mae_actual:.6f}\")\n",
    "        \n",
    "        if mae_actual < error_tolerado:\n",
    "            print(f\"Convergió en iteración {i} con MAE {mae_actual}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo MAE: {e}\")\n",
    "        break\n",
    "    \n",
    "    # El output de esta vuelta es el input de la siguiente\n",
    "    input_dir = output_dir\n",
    "\n",
    "print(success)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51df792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOB INTERMEDIO QUE SOLO CALCULA EL MAE ENTRE DOS ITERACIONES\n",
    "def map_mae(key, value, context):\n",
    "    # Asumiendo que la estructura de value es: [PP, {Retados}, PH_Nuevo, PH_Viejo]\n",
    "    # Si 'value' viene de disco como string, hay que parsearlo\n",
    "    datos = value.split(\"\\t\")\n",
    "        \n",
    "    ph_nuevo = float(datos[2])\n",
    "    ph_viejo = float(datos[3])\n",
    "    \n",
    "    # Calculamos el error absoluto: |PH_nuevo - PH_viejo|\n",
    "    error_absoluto = abs(ph_nuevo - ph_viejo)\n",
    "    \n",
    "    # Enviamos a una clave fija para que un Reducer calcule el promedio final\n",
    "    # Emitimos (error, 1) para poder calcular el promedio (Suma / Total)\n",
    "    context.write(\"GLOBAL_MAE\", (error_absoluto, 1))\n",
    "\n",
    "def red_mae(key, values, context):\n",
    "    suma_error_absoluto = 0\n",
    "    conteo_jugadores = 0\n",
    "    \n",
    "    for err, count in values:\n",
    "        suma_error_absoluto += err\n",
    "        conteo_jugadores += count\n",
    "    \n",
    "    # MAE = Suma de errores absolutos / N\n",
    "    mae_final = suma_error_absoluto / conteo_jugadores if conteo_jugadores > 0 else 0\n",
    "    \n",
    "    context.write(\"RESULTADO_MAE\", mae_final)\n",
    "\n",
    "\n",
    "# Parámetros iniciales\n",
    "alfa = 0.1\n",
    "error_tolerado = 0.001\n",
    "max_iter = 20\n",
    "input_dir = \"./format/\" # Salida del Job anterior (ID \\t [PP, PH])\n",
    "partidas_dir = \"./partidas/\"\n",
    "\n",
    "for i in range(max_iter):\n",
    "    output_dir = f\"./iteracion_{i}/\"\n",
    "    \n",
    "    # Configuramos el Job\n",
    "    job = Job(input_dir, output_dir, fmap, fred)\n",
    "    job.setParams(alfa)\n",
    "    job.setShuffleCmp(cmpShort)\n",
    "    job.setSortCmp(cmpShuffle)\n",
    "    \n",
    "    success = job.waitForCompletion()\n",
    "    \n",
    "    # --- CÁLCULO DE CONVERGENCIA (MSE) ---\n",
    "    mae_dir = f\"./mae_iter_{i}/\"\n",
    "    \n",
    "    # 1. Ejecutar Job de cálculo de PH (el que ya tienes)\n",
    "    # 2. Ejecutar Job de MAE\n",
    "    jobMAE = Job(output_dir, mae_dir, map_mae, red_mae)\n",
    "    jobMAE.waitForCompletion()\n",
    "    \n",
    "    # 3. Leer el resultado del MAE desde el Driver\n",
    "    # (El driver solo lee un archivo de texto de pocos bytes)\n",
    "    try:\n",
    "        with open(f\"{mae_dir}/output.txt\", \"r\") as f:\n",
    "            # Formato esperado: RESULTADO_MAE \\t 0.00045\n",
    "            linea = f.read().split(\"\\t\")\n",
    "            mae_actual = float(linea[1])\n",
    "            \n",
    "        print(f\">>> Iteración {i} - MAE: {mae_actual:.6f}\")\n",
    "        \n",
    "        if mae_actual < error_tolerado:\n",
    "            print(f\"Convergió en iteración {i} con MAE {mae_actual}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo MAE: {e}\")\n",
    "        break\n",
    "    \n",
    "    # El output de esta vuelta es el input de la siguiente\n",
    "    input_dir = output_dir\n",
    "\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e2373",
   "metadata": {},
   "source": [
    "Tercer Job saca top 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74dc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDir = \"./format/\"\n",
    "outputDir = \"./out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def fmap(key, value, context):\n",
    "    lista = value.split(\"\\t\")\n",
    "    context.write(1, (key, float(lista[-1])))\n",
    "\n",
    "def fcombiner(key, values, context):\n",
    "    # simple combiner: sum counts locally\n",
    "    top_maximo = []\n",
    "    \n",
    "    for v in values:\n",
    "        elemento = (v[1],v[0])\n",
    "        if len(top_maximo) < 10:\n",
    "            # Si hay menos de 10 elementos, simplemente lo añadimos al heap.\n",
    "            heapq.heappush(top_maximo, elemento)\n",
    "        else:\n",
    "            # Si el nuevo score (elemento[0]) es MAYOR que el elemento MÁS PEQUEÑO \n",
    "            # del heap (top_maximo[0]), reemplazamos el pequeño.\n",
    "            if elemento[0] > top_maximo[0][0]:\n",
    "                heapq.heapreplace(top_maximo, elemento)\n",
    "    top_10_final = sorted(top_maximo, key=lambda e: e[0], reverse=True)\n",
    "    for (maximo, clave) in top_10_final:\n",
    "        context.write(key, (clave,maximo))\n",
    "\n",
    "def fred(key, values, context):\n",
    "    # simple combiner: sum counts locally\n",
    "    top_maximo = []\n",
    "    \n",
    "    for v in values:\n",
    "        elemento = (v[1],v[0])\n",
    "        if len(top_maximo) < 10:\n",
    "            # Si hay menos de 10 elementos, simplemente lo añadimos al heap.\n",
    "            heapq.heappush(top_maximo, elemento)\n",
    "        else:\n",
    "            # Si el nuevo score (elemento[0]) es MAYOR que el elemento MÁS PEQUEÑO \n",
    "            # del heap (top_maximo[0]), reemplazamos el pequeño.\n",
    "            if elemento[0] > top_maximo[0][0]:\n",
    "                heapq.heapreplace(top_maximo, elemento)\n",
    "    top_10_final = sorted(top_maximo, key=lambda e: e[0], reverse=True)\n",
    "    for (maximo, clave) in top_10_final:\n",
    "        context.write(clave, maximo)\n",
    "\n",
    "jobCountBoth = Job(inputDir, outputDir, fmap, fred)\n",
    "jobCountBoth.setCombiner(fcombiner)\n",
    "success = jobCountBoth.waitForCompletion()\n",
    "\n",
    "print(success)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
